# -*- coding: utf-8 -*-
"""Hate Tweet Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pvuvaK4V1n8vVdv8oWZzrPi__He1BtYa
"""

import pandas as pd
from sklearn.model_selection import train_test_split
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from transformers import BertTokenizer, TFBertModel
import tensorflow_hub as hub
import tensorflow as tf

# Load dataset
df = pd.read_csv("/content/train_E6oV3lV-3.csv")
df

df.info()

df.describe()

nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\@\w+|\#','', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

df['cleaned_tweet'] = df['tweet'].apply(clean_text)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(df['cleaned_tweet'], df['label'], test_size=0.2, random_state=42)

# Bag-of-Words
vectorizer_bow = CountVectorizer()
X_train_bow = vectorizer_bow.fit_transform(X_train)
X_test_bow = vectorizer_bow.transform(X_test)

# TF-IDF
vectorizer_tfidf = TfidfVectorizer()
X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)
X_test_tfidf = vectorizer_tfidf.transform(X_test)

import numpy as np
from transformers import DistilBertTokenizer, TFDistilBertModel

# Using DistilBERT instead of BERT
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')

def encode_with_bert(texts, max_length=64, batch_size=32):
    encoded_outputs = []

    for i in range(0, len(texts), batch_size):
        # Processing in batches to reduce memory usage
        batch_texts = texts[i:i + batch_size]
        inputs = tokenizer(batch_texts.tolist(), return_tensors='tf', padding=True, truncation=True, max_length=max_length)

        # Extracting the last hidden state for all tokens
        outputs = bert_model(inputs)[0][:, 0, :]  # [batch_size, max_seq_len, hidden_size] -> [batch_size, hidden_size]

        encoded_outputs.append(outputs)

    # Concatenate all batch outputs into a single numpy array
    return np.concatenate(encoded_outputs, axis=0)

# Apply encoding to training and test data
X_train_bert = encode_with_bert(X_train, max_length=64, batch_size=32)
X_test_bert = encode_with_bert(X_test, max_length=64, batch_size=32)

# Universal Sentence Encoder
use_model = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

def embed_with_use(texts):
    return use_model(texts.tolist())

X_train_use = embed_with_use(X_train)
X_test_use = embed_with_use(X_test)

# Function to train and evaluate models
def train_evaluate_model(X_train, X_test, y_train, y_test):
    model = LogisticRegression(max_iter=1000)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

print("Bag-of-Words")
train_evaluate_model(X_train_bow, X_test_bow, y_train, y_test)

print("\nTF-IDF")
train_evaluate_model(X_train_tfidf, X_test_tfidf, y_train, y_test)

print("\nBERT")
train_evaluate_model(X_train_bert, X_test_bert, y_train, y_test)

print("\nUniversal Sentence Encoder")
train_evaluate_model(X_train_use, X_test_use, y_train, y_test)

import matplotlib.pyplot as plt

# Plotting performance comparison
plt.figure(figsize=(10, 6))
embeddings = list(results.keys())
accuracies = list(results.values())
plt.plot(embeddings, accuracies, marker='o', linestyle='-', color='b')
plt.title('Model Performance Comparison')
plt.xlabel('Text Embedding')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.grid(True)
plt.show()